{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "474a4713",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-16T13:18:38.452053Z",
     "iopub.status.busy": "2022-02-16T13:18:38.451477Z",
     "iopub.status.idle": "2022-02-16T13:21:14.388288Z",
     "shell.execute_reply": "2022-02-16T13:21:14.386993Z",
     "shell.execute_reply.started": "2022-02-16T13:18:38.451954Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yaml\n",
    "from geospark.register import GeoSparkRegistrator\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import DataFrame, SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import FloatType, IntegerType, LongType\n",
    "from pyspark.sql.window import Window\n",
    "from tqdm import tqdm\n",
    "\n",
    "config = yaml.load(open(r\"config/config.yaml\"), Loader=yaml.FullLoader)\n",
    "conf = SparkConf().setAll(config[\"sparkConf\"].items())\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"DistancesDistribution\")\n",
    "    .config(conf=conf)\n",
    "    .enableHiveSupport()\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "GeoSparkRegistrator.registerAll(spark)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from lib.distribution import *\n",
    "from lib.utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb04ddb",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "727b9dfd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-16T13:50:55.498307Z",
     "iopub.status.busy": "2022-02-16T13:50:55.497733Z",
     "iopub.status.idle": "2022-02-16T13:50:55.839861Z",
     "shell.execute_reply": "2022-02-16T13:50:55.837873Z",
     "shell.execute_reply.started": "2022-02-16T13:50:55.498242Z"
    }
   },
   "outputs": [],
   "source": [
    "# Features table and features\n",
    "categoricalFeatures = None\n",
    "numericalFeatures = [\"distance\"]\n",
    "categoricalFunctions = None\n",
    "numericalFunctions = [\"min\"]\n",
    "\n",
    "grid = spark.table(config[\"tableConf\"][\"gridCentroids\"])\n",
    "dataFeatures = (\n",
    "    spark.table(config[\"tableConf\"][\"locality_with_centroids\"])\n",
    "    .dropDuplicates([\"locality_id\"])\n",
    "    .na.fill(0, numericalFeatures)\n",
    ")\n",
    "\n",
    "gridID = \"gid\"\n",
    "gridGeometry = \"geom_wkt\"\n",
    "featuresGeometry = \"geometry\"\n",
    "gridCentrLon = \"lon\"\n",
    "gridCentrLat = \"lat\"\n",
    "featuresCentrLon = \"lon\"\n",
    "featuresCentrLat = \"lat\"\n",
    "\n",
    "featurePopulation = \"locality_population\"\n",
    "binPopulation = \"bin_population\"\n",
    "\n",
    "toWriteTbl = f\"{dbName}.{prefixName}_geo_grid_distances\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31444d2",
   "metadata": {},
   "source": [
    "## Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a2f9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "gridDistrib = GridDistribution(\n",
    "    gridData,\n",
    "    gridID,\n",
    "    gridGeometry,\n",
    "    dataFeatures,\n",
    "    featuresGeometry,\n",
    "    numericalFeatures,\n",
    "    categoricalFeatures,\n",
    "    categoricalFunctions,\n",
    "    numericalFunctions,\n",
    ")\n",
    "\n",
    "saveTableOverwritePartition(\n",
    "   gridDistrib.gridDistances(\n",
    "       featurePopulation,\n",
    "       binPopulation,\n",
    "       gridCentrLon,\n",
    "       gridCentrLat,\n",
    "       featuresCentrLon,\n",
    "       featuresCentrLat,\n",
    "   ),\n",
    "   50,\n",
    "   [gridID],\n",
    "   toWriteTbl,\n",
    "   \"overwrite\",\n",
    ")\n",
    "\n",
    "allDistancesData = spark.table(toWriteTbl)\n",
    "\n",
    "binsPop = allDistancesData.select(\n",
    "    collect_set(col(f\"{binPopulation}\")).alias(f\"{binPopulation}\")\n",
    ").first()[f\"{binPopulation}\"]\n",
    "\n",
    "distancesTblAndAliases = {}\n",
    "\n",
    "for binPop in binsPop:\n",
    "    distanseData = allDistancesData.filter(col(f\"{binPopulation}\") == binPop)\n",
    "    distancesTblAndAliases[distanseData] = f\"lower_{binPop}\"\n",
    "\n",
    "toWriteTbl = f\"{dbName}.{prefixName}_geo_grid_locality_distances\"\n",
    "\n",
    "DistancesJoining = gridDistrib.joining(distancesTblAndAliases)\n",
    "\n",
    "saveTableOverwritePartition(\n",
    "    DistancesJoining,\n",
    "    135,\n",
    "    [gridID],\n",
    "    toWriteTbl,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c029aad",
   "metadata": {},
   "source": [
    "## Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5148631d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas\n",
    "import keplergl\n",
    "from shapely import wkt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f17b2b",
   "metadata": {},
   "source": [
    "### Take only one region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a504a604",
   "metadata": {},
   "outputs": [],
   "source": [
    "regionID = # insert your value\n",
    "featuresDF = spark.table(f\"{dbName}.{prefixName}_geo_grid_locality_distances\")\n",
    "gridLocal = grid.filter(col(\"region_id\") == regionID)\n",
    "\n",
    "featuresDF = (\n",
    "    featuresDF\n",
    "    .join(gridLocal, [f\"{gridID}\"], \"inner\")\n",
    "    .withColumnRenamed(f\"{gridGeometry}\", \"geometry\")\n",
    ")\n",
    "\n",
    "print(featuresDF.count())\n",
    "\n",
    "df = featuresDF.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403cd71e",
   "metadata": {},
   "source": [
    "### Transform to geopandas and add to map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6aa3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df.columns:\n",
    "    if column != \"geometry\":\n",
    "        df[column] = df[column].astype(\"int\")\n",
    "\n",
    "\n",
    "df[\"geometry\"] = df.apply(lambda x: wkt.loads(str(x[\"geometry\"])), axis=1)\n",
    "\n",
    "poly_sectors_gdf = geopandas.GeoDataFrame(\n",
    "    df, crs={\"init\": \"epsg:4326\"}, geometry=\"geometry\"\n",
    ")\n",
    "\n",
    "map_1 = keplergl.KeplerGl(height=900)\n",
    "map_1.add_data(\n",
    "    data=poly_sectors_gdf\n",
    ")\n",
    "map_1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Geo",
   "language": "python",
   "name": "geo_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
