{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bdd1ffd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-21T11:15:44.319270Z",
     "iopub.status.busy": "2022-02-21T11:15:44.318481Z",
     "iopub.status.idle": "2022-02-21T11:16:00.482392Z",
     "shell.execute_reply": "2022-02-21T11:16:00.481038Z",
     "shell.execute_reply.started": "2022-02-21T11:15:44.319210Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m config \u001b[38;5;241m=\u001b[39m yaml\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig/testconfig.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m), Loader\u001b[38;5;241m=\u001b[39myaml\u001b[38;5;241m.\u001b[39mFullLoader)\n\u001b[1;32m     13\u001b[0m conf \u001b[38;5;241m=\u001b[39m SparkConf()\u001b[38;5;241m.\u001b[39msetAll(config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparkConf\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mitems())\n\u001b[1;32m     15\u001b[0m spark \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m---> 16\u001b[0m     \u001b[43mSparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mOSMDistribution\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menableHiveSupport\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m )\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m#GeoSparkRegistrator.registerAll(spark)\u001b[39;00m\n\u001b[1;32m     24\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mload_ext\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mautoreload\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/geo/lib/python3.8/site-packages/pyspark/sql/session.py:228\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    226\u001b[0m         sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[0;32m--> 228\u001b[0m     sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[1;32m    231\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc)\n",
      "File \u001b[0;32m~/.conda/envs/geo/lib/python3.8/site-packages/pyspark/context.py:384\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 384\u001b[0m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",
      "File \u001b[0;32m~/.conda/envs/geo/lib/python3.8/site-packages/pyspark/context.py:146\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    144\u001b[0m SparkContext\u001b[38;5;241m.\u001b[39m_ensure_initialized(\u001b[38;5;28mself\u001b[39m, gateway\u001b[38;5;241m=\u001b[39mgateway, conf\u001b[38;5;241m=\u001b[39mconf)\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 146\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaster\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mappName\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparkHome\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpyFiles\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menvironment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatchSize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserializer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjsc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofiler_cls\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;66;03m# If an error occurs, clean up in order to allow future SparkContext creation:\u001b[39;00m\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop()\n",
      "File \u001b[0;32m~/.conda/envs/geo/lib/python3.8/site-packages/pyspark/context.py:209\u001b[0m, in \u001b[0;36mSparkContext._do_init\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvironment[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPYTHONHASHSEED\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPYTHONHASHSEED\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    208\u001b[0m \u001b[38;5;66;03m# Create the Java SparkContext through Py4J\u001b[39;00m\n\u001b[0;32m--> 209\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc \u001b[38;5;241m=\u001b[39m jsc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize_context\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;66;03m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001b[39;00m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conf \u001b[38;5;241m=\u001b[39m SparkConf(_jconf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc\u001b[38;5;241m.\u001b[39msc()\u001b[38;5;241m.\u001b[39mconf())\n",
      "File \u001b[0;32m~/.conda/envs/geo/lib/python3.8/site-packages/pyspark/context.py:321\u001b[0m, in \u001b[0;36mSparkContext._initialize_context\u001b[0;34m(self, jconf)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_initialize_context\u001b[39m(\u001b[38;5;28mself\u001b[39m, jconf):\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;124;03m    Initialize SparkContext in function to allow subclass specific initialization\u001b[39;00m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 321\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mJavaSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjconf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/geo/lib/python3.8/site-packages/py4j/java_gateway.py:1572\u001b[0m, in \u001b[0;36mJavaClass.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1564\u001b[0m args_command \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[1;32m   1565\u001b[0m     [get_command_part(arg, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m new_args])\n\u001b[1;32m   1567\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCONSTRUCTOR_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1568\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_command_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1569\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1570\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1572\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1573\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1574\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_client, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fqn)\n\u001b[1;32m   1576\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/.conda/envs/geo/lib/python3.8/site-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m~/.conda/envs/geo/lib/python3.8/site-packages/py4j/java_gateway.py:1205\u001b[0m, in \u001b[0;36mGatewayConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1201\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JNetworkError(\n\u001b[1;32m   1202\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError while sending\u001b[39m\u001b[38;5;124m\"\u001b[39m, e, proto\u001b[38;5;241m.\u001b[39mERROR_ON_SEND)\n\u001b[1;32m   1204\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1205\u001b[0m     answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m   1206\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m   1207\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m answer\u001b[38;5;241m.\u001b[39mstartswith(proto\u001b[38;5;241m.\u001b[39mRETURN_MESSAGE):\n",
      "File \u001b[0;32m~/.conda/envs/geo/lib/python3.8/socket.py:669\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    668\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 669\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    670\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    671\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yaml\n",
    "#from geospark.register import GeoSparkRegistrator\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import DataFrame, SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import FloatType, IntegerType, LongType\n",
    "from pyspark.sql.window import Window\n",
    "from tqdm import tqdm\n",
    "\n",
    "config = yaml.load(open(r\"config/testconfig.yaml\"), Loader=yaml.FullLoader)\n",
    "conf = SparkConf().setAll(config[\"sparkConf\"].items())\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"OSMDistribution\")\n",
    "    .config(conf=conf)\n",
    "    .enableHiveSupport()\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "#GeoSparkRegistrator.registerAll(spark)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from lib.osmFeatures import *\n",
    "from lib.distribution import *\n",
    "from lib.utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8e42a3",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ae92077",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-16T13:50:55.498307Z",
     "iopub.status.busy": "2022-02-16T13:50:55.497733Z",
     "iopub.status.idle": "2022-02-16T13:50:55.839861Z",
     "shell.execute_reply": "2022-02-16T13:50:55.837873Z",
     "shell.execute_reply.started": "2022-02-16T13:50:55.498242Z"
    }
   },
   "outputs": [],
   "source": [
    "# Features table and features\n",
    "osmFeatureSpecification = [\n",
    "    \"lowLvlBuild\",\n",
    "    \"middleLvlBuild\",\n",
    "    \"highLvlBuild\",\n",
    "    \"lvlBuild\",\n",
    "    \"allBuild\",\n",
    "    \"adminBuild\",\n",
    "    \"industrialObj\",\n",
    "    \"publicTransportObj\",\n",
    "    \"residentObj\",\n",
    "    \"medsObj\",\n",
    "    \"hotelObj\",\n",
    "    \"entertainObj\",\n",
    "    \"retailObj\",\n",
    "    \"religiousObj\",\n",
    "    \"histObj\",\n",
    "    \"parkObj\",\n",
    "    \"sportObj\",\n",
    "    \"campObj\",\n",
    "    \"beachObj\",\n",
    "    \"foodObj\",\n",
    "    \"militaryObj\",\n",
    "    \"prisonObj\",\n",
    "    \"parkingObj\",\n",
    "    \"roadObj\",\n",
    "]\n",
    "\n",
    "grid = spark.table(config[\"tableConf\"][\"gridBuffers\"])\n",
    "\n",
    "osmGeometry = spark.table(config[\"tableConf\"][\"osmGeometryTableName\"]).filter(\n",
    "    col(\"load_date\") == config[\"tableConf\"][\"osmLoadDate\"]\n",
    ")\n",
    "\n",
    "osmTag = spark.table(config[\"tableConf\"][\"osmTagTableName\"]).filter(\n",
    "    col(\"load_date\") == config[\"tableConf\"][\"osmLoadDate\"]\n",
    ")\n",
    "\n",
    "allOsmObjectsTbl = f\"{dbName}.{prefixName}_all_osm_objects\"\n",
    "osmObjectsTbl = f\"{dbName}.{prefixName}_osm_objects\"\n",
    "osmID = [\"osm_id\", \"osm_type\"]\n",
    "\n",
    "allOsmObjects = getOsmObjects(osmGeometry, osmTag, osmFeatureSpecification)\n",
    "\n",
    "saveTableOverwritePartition(allOsmObjects, 50, osmID, allOsmObjectsTbl)\n",
    "\n",
    "osmObjects = (\n",
    "    spark.table(allOsmObjectsTbl)\n",
    "    .na.drop(how=\"all\", subset=osmFeatureSpecification)\n",
    ")\n",
    "\n",
    "saveTableOverwritePartition(osmObjects, 50, osmID, osmObjectsTbl)\n",
    "\n",
    "featuresTable = f\"{dbName}.{prefixName}_osm_objects\"\n",
    "\n",
    "featuresGeometry = \"osm_geom_wkt\"\n",
    "gridID = \"gid\"\n",
    "gridGeometry = \"geom_wkt\"\n",
    "\n",
    "categoricalFeatures = None\n",
    "\n",
    "# Dictionary tables: which cell buffer intersect which cell\n",
    "buffersDicts = {\n",
    "    \"\": None,\n",
    "    spark.table(f\"{dbName}.{prefixName}_grid_dict_buffer_1km\"): \"buffer_1km\",\n",
    "    spark.table(f\"{dbName}.{prefixName}_grid_dict_buffer_2km\"): \"buffer_2km\",\n",
    "    spark.table(f\"{dbName}.{prefixName}_grid_dict_buffer_3km\"): \"buffer_3km\",\n",
    "    spark.table(f\"{dbName}.{prefixName}_grid_dict_buffer_4km\"): \"buffer_4km\",\n",
    "    spark.table(f\"{dbName}.{prefixName}_grid_dict_buffer_5km\"): \"buffer_5km\",\n",
    "}\n",
    "\n",
    "# Tables for writing and their aliases\n",
    "tblAndAliases = {\n",
    "    f\"{dbName}.{prefixName}_grid_osm_grid\": \"grid\",\n",
    "    f\"{dbName}.{prefixName}_grid_osm_1km\": \"1km\",\n",
    "    f\"{dbName}.{prefixName}_grid_osm_2km\": \"2km\",\n",
    "    f\"{dbName}.{prefixName}_grid_osm_3km\": \"3km\",\n",
    "    f\"{dbName}.{prefixName}_grid_osm_4km\": \"4km\",\n",
    "    f\"{dbName}.{prefixName}_grid_osm_5km\": \"5km\"}\n",
    "\n",
    "toWriteTbls = list(tblAndAliases.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f8b926",
   "metadata": {},
   "source": [
    "## Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4e55d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tblNumber, bufferTbl in enumerate(buffersDicts.keys()):  \n",
    "    print(bufferTbl)\n",
    "    \n",
    "    bufferData = bufferTbl\n",
    "    bufferColumn = buffersDicts[bufferTbl]\n",
    "    \n",
    "    for featureNumber, feature in enumerate(osmFeatureSpecification): \n",
    "        \n",
    "        numericalFeatures = [feature]\n",
    "    \n",
    "        dataFeatures = (\n",
    "            spark.table(featuresTable)\n",
    "            .dropDuplicates([\"osm_id\", \"osm_type\"])\n",
    "            .filter(col(f\"{feature}\") > 0)\n",
    "            .select(col(f\"{feature}\"), col(f\"{featuresGeometry}\"))\n",
    "            )\n",
    "        \n",
    "        gridDistrib = GridDistribution(\n",
    "            grid,\n",
    "            gridID,\n",
    "            gridGeometry,\n",
    "            dataFeatures,\n",
    "            featuresGeometry,\n",
    "            numericalFeatures,\n",
    "            categoricalFeatures,\n",
    "            categoricalFunctions=None,\n",
    "            numericalFunctions=[\"sum\"]\n",
    "        )\n",
    "        \n",
    "        featuresAndGrid = gridDistrib.featuresByGrid()\n",
    "\n",
    "        gridDistribFeatureLocality = gridDistrib.gridFeaturesAgg(\n",
    "            featuresAndGrid, bufferData, bufferColumn\n",
    "        )\n",
    "        \n",
    "        if featureNumber == 0:\n",
    "            gridDistribLocality = gridDistribFeatureLocality\n",
    "        else:\n",
    "            gridDistribLocality = gridDistribLocality.join(gridDistribFeatureLocality, gridID, \"outer\")\n",
    "    \n",
    "        \n",
    "    saveTableOverwritePartition(\n",
    "        gridDistribLocality, 50, [gridID], toWriteTbls[tblNumber]\n",
    "    )\n",
    "\n",
    "toWriteTbl = f\"{dbName}.{prefixName}_geo_grid_osm\"\n",
    "\n",
    "OSMJoining = gridDistrib.joining(tblAndAliases)\n",
    "\n",
    "saveTableOverwritePartition(\n",
    "    OSMJoining,\n",
    "    135,\n",
    "    [gridID],\n",
    "    toWriteTbl,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc04136",
   "metadata": {},
   "source": [
    "## Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34749b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas\n",
    "import keplergl\n",
    "from shapely import wkt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc78f83",
   "metadata": {},
   "source": [
    "### Take only one region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab2f8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "regionID = # insert your value\n",
    "featuresDF = spark.table(f\"{dbName}.{prefixName}_geo_grid_osm\")\n",
    "gridLocal = grid.filter(col(\"region_id\") == regionID)\n",
    "\n",
    "featuresDF = (\n",
    "    featuresDF\n",
    "    .join(gridLocal, [f\"{gridID}\"], \"inner\")\n",
    "    .withColumnRenamed(f\"{gridGeometry}\", \"geometry\")\n",
    ")\n",
    "\n",
    "print(featuresDF.count())\n",
    "\n",
    "df = featuresDF.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2ddcd4",
   "metadata": {},
   "source": [
    "### Transform to geopandas and add to map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592950bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df.columns:\n",
    "    if column != \"geometry\":\n",
    "        df[column] = df[column].astype(\"int\")\n",
    "\n",
    "\n",
    "df[\"geometry\"] = df.apply(lambda x: wkt.loads(str(x[\"geometry\"])), axis=1)\n",
    "\n",
    "poly_sectors_gdf = geopandas.GeoDataFrame(\n",
    "    df, crs={\"init\": \"epsg:4326\"}, geometry=\"geometry\"\n",
    ")\n",
    "\n",
    "map_1 = keplergl.KeplerGl(height=900)\n",
    "map_1.add_data(\n",
    "    data=poly_sectors_gdf\n",
    ")\n",
    "map_1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Geo (Python)",
   "language": "python",
   "name": "geo_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
